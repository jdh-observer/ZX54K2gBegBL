# -*- coding: utf-8 -*-
"""Code Snippet 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EU1ya6U3dQ43KXneEdgR41KQP804AW1o
"""

"""
Code Snippet 3 — Validation checks for missingness and precision flags (Rewritten, v2)

Purpose (JDH 4.3):
- Derive explicit temporal/spatial precision classes WITHOUT altering harmonized values
- Validate internal consistency against the project’s precision rubric
- Audit missingness as an evidentiary condition (not a defect)
- Export an evidence log + reproducible sample rows + validation summary

Input:
- harmonized_geocoded.csv (output of Code Snippet 1 + 2)

Outputs:
- evidence_log.csv
- table4_evidence_log_sample_rows.csv
- validation_summary.json
- validation_issues.csv (if any errors/warnings)

Notes (hard constraints):
- This script MUST NOT:
  - geocode anything
  - change lat/lon, decades_norm, start_year/end_year, etc.
  - merge/cluster records across sources
- This script MAY:
  - normalize categorical enums for auditability (e.g., geocode_status casing / missing encoding)
    because it does not change spatial claims or derivations, only representation.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd


# =========================
# Config
# =========================

IN_PATH = Path("/content/harmonized_geocoded.csv")  # adjust if needed
OUT_DIR = Path("/content/")
OUT_DIR.mkdir(parents=True, exist_ok=True)

OUT_EVIDENCE_LOG = OUT_DIR / "evidence_log.csv"
OUT_TABLE4_SAMPLES = OUT_DIR / "table4_evidence_log_sample_rows.csv"
OUT_VALIDATION_SUMMARY = OUT_DIR / "validation_summary.json"
OUT_VALIDATION_ISSUES = OUT_DIR / "validation_issues.csv"

# Table 4 sampling
SAMPLES_PER_KEY_BRANCH = 2  # 1–2 rows per branch
RANDOM_SEED = 42

# Distribution missing token (for JSON summary only)
MISSING_TOKEN = "<missing>"


# =========================
# Helpers
# =========================

def norm_ws(x: Any) -> Optional[str]:
    if x is None or (isinstance(x, float) and np.isnan(x)):
        return None
    s = str(x).strip()
    return s if s else None


def is_missing(x: Any) -> bool:
    return (
        x is None
        or (isinstance(x, float) and np.isnan(x))
        or (isinstance(x, str) and x.strip() == "")
    )


def safe_json_loads_list(x: Any) -> Optional[List[Any]]:
    """Parse a JSON-serialized list (e.g., '[]', '["1930s","1940s"]'). Return None if not parseable."""
    if is_missing(x):
        return None
    if isinstance(x, list):
        return x
    s = str(x).strip()
    try:
        v = json.loads(s)
        return v if isinstance(v, list) else None
    except Exception:
        return None


def now_iso() -> str:
    return pd.Timestamp.utcnow().isoformat()


def require_cols(df: pd.DataFrame, cols: List[str]) -> List[str]:
    return [c for c in cols if c not in df.columns]


def as_bool_series(s: pd.Series) -> pd.Series:
    """Robust boolean conversion (accept True/False, 1/0, 'true'/'false')."""
    if s.dtype == bool:
        return s
    return s.astype(str).str.strip().str.lower().isin(["true", "1", "yes", "y"])


def json_list_str(items: List[str]) -> str:
    return json.dumps(items, ensure_ascii=False)


def normalize_geocode_status(x: Any) -> Optional[str]:
    """
    Normalize geocode_status to a controlled lowercase vocabulary.
    Representation fix only: does not change geocoding outcomes or spatial claims.
    """
    if is_missing(x):
        return None
    s = str(x).strip().lower()

    mapping = {
        "match": "match",
        "matched": "match",
        "no_match": "no_match",
        "no match": "no_match",
        "nomatch": "no_match",
        "skipped": "skipped",
        "skip": "skipped",
        "error": "error",
        "failed": "error",
        "not_attempted": "not_attempted",
        "not attempted": "not_attempted",
    }
    return mapping.get(s, s)  # preserve unknowns for warnings


def value_counts_for_json(s: pd.Series) -> Dict[str, int]:
    """
    Stable counts for summary JSON without turning NaN into string 'nan'.
    """
    return (
        s.fillna(MISSING_TOKEN)
         .value_counts(dropna=False)
         .astype(int)
         .to_dict()
    )


# =========================
# Step 3.1 — Derivation
# =========================

def derive_time_precision_class(df: pd.DataFrame) -> pd.Series:
    """
    Output classes:
      - decade: decades_norm non-empty list
      - year_range: start_year and/or end_year present but decades_norm empty/none
      - vague: textual time hints exist but not structured (optional; requires a true time-hint field)
      - missing: no decade + no start/end + no time signals

    Important fix:
      - Do NOT treat "active_raw" as a time hint (it caused a small set of false 'vague' labels).
    """
    decades_list = df.get("decades_norm", pd.Series([None] * len(df))).apply(safe_json_loads_list)
    has_decade = decades_list.apply(lambda v: isinstance(v, list) and len(v) > 0)

    start = pd.to_numeric(df.get("start_year", pd.Series([np.nan] * len(df))), errors="coerce")
    end = pd.to_numeric(df.get("end_year", pd.Series([np.nan] * len(df))), errors="coerce")
    has_year_bounds = (~pd.isna(start)) | (~pd.isna(end))

    # Conservative textual time hints (do NOT include active_raw)
    time_text_fields = [c for c in ["time_note", "time_text", "years_raw"] if c in df.columns]
    has_time_text_hint = pd.Series([False] * len(df), index=df.index)
    for c in time_text_fields:
        has_time_text_hint = has_time_text_hint | df[c].apply(lambda x: not is_missing(x))

    out = pd.Series(["missing"] * len(df), index=df.index)
    out.loc[has_decade] = "decade"
    out.loc[(~has_decade) & has_year_bounds] = "year_range"
    out.loc[(~has_decade) & (~has_year_bounds) & has_time_text_hint] = "vague"
    return out


def derive_spatial_precision_class(df: pd.DataFrame) -> pd.Series:
    """
    Output classes (suggested):
      - point
      - intersection
      - corridor_or_neighborhood
      - missing
    """
    lat = pd.to_numeric(df.get("lat", pd.Series([np.nan] * len(df))), errors="coerce")
    lon = pd.to_numeric(df.get("lon", pd.Series([np.nan] * len(df))), errors="coerce")
    has_coords = (~pd.isna(lat)) & (~pd.isna(lon))

    address_kind = df.get("address_kind", pd.Series([None] * len(df))).apply(norm_ws).fillna("unknown").str.lower()

    is_intersection = address_kind.isin(["intersection"])
    is_nonpointable = address_kind.isin(["between_phrase", "descriptive", "multi_address"])

    out = pd.Series(["missing"] * len(df), index=df.index)
    out.loc[is_intersection] = "intersection"
    out.loc[is_nonpointable] = "corridor_or_neighborhood"
    out.loc[(out == "missing") & has_coords] = "point"
    return out


def derive_corroboration_status(df: pd.DataFrame) -> pd.Series:
    """
    Conservative corroboration indicator.
    Default: 'single_source'
    If candidate_n_sources exists: mark >=2 as 'multi_source_candidate' without asserting equivalence.
    """
    if "candidate_n_sources" in df.columns:
        n = pd.to_numeric(df["candidate_n_sources"], errors="coerce").fillna(1)
        out = pd.Series(["single_source"] * len(df), index=df.index)
        out.loc[n >= 2] = "multi_source_candidate"
        return out
    return pd.Series(["single_source"] * len(df), index=df.index)


def add_manual_audit_control_fields(df: pd.DataFrame) -> pd.DataFrame:
    """
    Add minimal manual-audit control fields.
    These do not change harmonized values; they only scaffold human review.
    """
    if "audit_stage" not in df.columns:
        df["audit_stage"] = "S0_not_reviewed"

    if "audit_decision" not in df.columns:
        df["audit_decision"] = "pending"  # pending/keep/drop/needs_review

    if "audit_reason" not in df.columns:
        df["audit_reason"] = None  # controlled vocab later

    # audit_flags as JSON list (derived, non-interpretive)
    if "audit_flags" not in df.columns:
        flags: List[List[str]] = []

        for _, r in df.iterrows():
            f: List[str] = []

            # Time flags
            tpc = str(r.get("time_precision_class", "")).strip().lower()
            if tpc == "missing":
                f.append("time_missing")
            elif tpc == "vague":
                f.append("time_vague")
            elif tpc == "year_range":
                f.append("time_year_range")

            # Space flags
            spc = str(r.get("spatial_precision_class", "")).strip().lower()
            if spc == "missing":
                f.append("space_missing")
            elif spc == "intersection":
                f.append("space_intersection")
            elif spc == "corridor_or_neighborhood":
                f.append("space_nonpointable")

            # Geocode trace flags
            gs = str(r.get("geocode_status", "")).strip().lower()
            attempted = r.get("geocode_attempted", None)
            attempted_b = bool(attempted) if isinstance(attempted, (bool, np.bool_)) else str(attempted).strip().lower() in ["true", "1", "yes", "y"]

            if attempted_b:
                f.append("geocode_attempted")

            # Use normalized statuses
            if gs == "no_match":
                f.append("geocode_no_match")
            elif gs == "error":
                f.append("geocode_error")
            elif gs == "skipped":
                f.append("geocode_skipped")
            elif gs == "not_attempted":
                f.append("geocode_not_attempted")

            flags.append(f)

        df["audit_flags"] = [json_list_str(x) for x in flags]

    return df


# =========================
# Step 3.2 — Consistency checks
# =========================

def validate_enums(df: pd.DataFrame) -> List[Dict[str, Any]]:
    issues: List[Dict[str, Any]] = []

    expected_time_basis = {
        "s1.decades_reported", "s1.decades_derived_from_bounds", "s1.decades_missing",
        "s2.decades_reported", "s2.decades_derived_from_bounds", "s2.decades_missing",
        "s3.decades_reported", "s3.decades_derived_from_bounds", "s3.decades_missing",
        "decades_reported", "decades_derived_from_bounds", "decades_missing",
        None
    }
    expected_address_kind = {
        "street_address", "intersection", "between_phrase", "multi_address", "descriptive", "missing", "unknown", None
    }
    expected_geocode_status = {
        "match", "no_match", "skipped", "error", "not_attempted", None
    }

    def check_enum(col: str, expected: set, severity: str):
        if col not in df.columns:
            return

        # NOTE: df[col] may be object with None; norm_ws returns None for blanks
        vals = df[col].apply(norm_ws)
        vals_norm = vals.apply(lambda x: x.lower() if isinstance(x, str) else None)

        allowed = {(v.lower() if isinstance(v, str) else v) for v in expected if v is not None} | {None}
        bad = ~vals_norm.isin(allowed)

        for i in df.index[bad.fillna(False)].tolist()[:50]:
            issues.append({
                "severity": severity,
                "issue_type": "enum_unexpected",
                "column": col,
                "entry_id": df.at[i, "entry_id"] if "entry_id" in df.columns else None,
                "value": df.at[i, col],
                "message": f"Unexpected enum value in {col}: {df.at[i, col]!r}"
            })

    check_enum("time_basis", expected_time_basis, "warning")
    check_enum("address_kind", expected_address_kind, "warning")
    check_enum("geocode_status", expected_geocode_status, "warning")
    return issues


def validate_logic(df: pd.DataFrame) -> List[Dict[str, Any]]:
    issues: List[Dict[str, Any]] = []

    if "entry_id" not in df.columns:
        return [{
            "severity": "error",
            "issue_type": "missing_column",
            "column": "entry_id",
            "message": "Missing required column: entry_id"
        }]

    lat = pd.to_numeric(df.get("lat", pd.Series([np.nan] * len(df))), errors="coerce")
    lon = pd.to_numeric(df.get("lon", pd.Series([np.nan] * len(df))), errors="coerce")
    has_coords = (~pd.isna(lat)) & (~pd.isna(lon))

    # 1) time_basis vs decades_norm coherence
    if "time_basis" in df.columns and "decades_norm" in df.columns:
        decades_list = df["decades_norm"].apply(safe_json_loads_list)
        has_decade = decades_list.apply(lambda v: isinstance(v, list) and len(v) > 0)
        tb = df["time_basis"].apply(norm_ws).fillna("")
        tb_missing = tb.str.contains("missing", case=False, na=False)

        bad = tb_missing & has_decade
        for i in df.index[bad].tolist()[:200]:
            issues.append({
                "severity": "error",
                "issue_type": "time_basis_conflict",
                "entry_id": df.at[i, "entry_id"],
                "message": "time_basis indicates missing but decades_norm is non-empty",
            })

    # 2) geocode_attempted True but geocode_status empty -> error
    if "geocode_attempted" in df.columns and "geocode_status" in df.columns:
        attempted = as_bool_series(df["geocode_attempted"])
        status_missing = df["geocode_status"].apply(is_missing)
        bad = attempted & status_missing
        for i in df.index[bad].tolist()[:200]:
            issues.append({
                "severity": "error",
                "issue_type": "geocode_trace_incomplete",
                "entry_id": df.at[i, "entry_id"],
                "message": "geocode_attempted=True but geocode_status is missing",
            })

    # 3) address_kind != street_address but geocode_attempted=True -> warning
    if "address_kind" in df.columns and "geocode_attempted" in df.columns:
        kind = df["address_kind"].apply(norm_ws).fillna("unknown").str.lower()
        attempted = as_bool_series(df["geocode_attempted"])
        bad = attempted & (~kind.isin(["street_address"]))
        for i in df.index[bad].tolist()[:200]:
            issues.append({
                "severity": "warning",
                "issue_type": "geocode_attempt_non_street_address",
                "entry_id": df.at[i, "entry_id"],
                "message": f"geocode_attempted=True but address_kind={df.at[i, 'address_kind']!r}",
            })

    # 4) spatial_precision_class=point but address_kind non-pointable -> error
    if "spatial_precision_class" in df.columns and "address_kind" in df.columns:
        sp = df["spatial_precision_class"].apply(norm_ws).fillna("").str.lower()
        kind = df["address_kind"].apply(norm_ws).fillna("unknown").str.lower()
        nonpointable = kind.isin(["descriptive", "between_phrase", "multi_address"])
        bad = (sp == "point") & nonpointable
        for i in df.index[bad].tolist()[:200]:
            issues.append({
                "severity": "error",
                "issue_type": "over_pointification",
                "entry_id": df.at[i, "entry_id"],
                "message": "spatial_precision_class=point but address_kind is non-pointable",
            })

    # 5) spatial_precision_class missing but coords present -> warning
    if "spatial_precision_class" in df.columns:
        sp = df["spatial_precision_class"].apply(norm_ws)
        bad = sp.apply(is_missing) & has_coords
        for i in df.index[bad].tolist()[:200]:
            issues.append({
                "severity": "warning",
                "issue_type": "precision_missing_with_coords",
                "entry_id": df.at[i, "entry_id"],
                "message": "Coordinates present but spatial_precision_class missing",
            })

    return issues


# =========================
# Step 3.3 — Missingness audit
# =========================

def missingness_audit(df: pd.DataFrame) -> Dict[str, Any]:
    n = len(df)
    out: Dict[str, Any] = {"n_records": int(n)}

    lat = pd.to_numeric(df.get("lat", np.nan), errors="coerce")
    lon = pd.to_numeric(df.get("lon", np.nan), errors="coerce")
    has_coords = (~pd.isna(lat)) & (~pd.isna(lon))
    out["coords_missing_n"] = int((~has_coords).sum())
    out["coords_missing_rate"] = float((~has_coords).mean()) if n else 0.0

    if "decades_norm" in df.columns:
        decades_list = df["decades_norm"].apply(safe_json_loads_list)
        has_decade = decades_list.apply(lambda v: isinstance(v, list) and len(v) > 0)
        out["decade_missing_n"] = int((~has_decade).sum())
        out["decade_missing_rate"] = float((~has_decade).mean()) if n else 0.0

    # Explainability checks (conservative)
    if "geocode_skip_reason" in df.columns:
        skip_reason_present = df["geocode_skip_reason"].apply(lambda x: not is_missing(x))
    else:
        skip_reason_present = pd.Series([False] * n, index=df.index)

    if "space_basis" in df.columns:
        space_basis = df["space_basis"].apply(norm_ws).fillna("").str.lower()
    else:
        space_basis = pd.Series([""] * n, index=df.index)

    coords_missing = ~has_coords
    explainable_coords_missing = coords_missing & (
        skip_reason_present | space_basis.isin(["address_only", "non_pointable", "unknown"])
    )
    out["coords_missing_explainable_n"] = int(explainable_coords_missing.sum())
    out["coords_missing_explainable_rate"] = float(explainable_coords_missing.mean()) if n else 0.0

    if "time_basis" in df.columns and "decades_norm" in df.columns:
        tb = df["time_basis"].apply(norm_ws).fillna("").str.lower()
        decades_list = df["decades_norm"].apply(safe_json_loads_list)
        has_decade = decades_list.apply(lambda v: isinstance(v, list) and len(v) > 0)
        decade_missing = ~has_decade
        explainable_decade_missing = decade_missing & tb.str.contains("missing|vague|derived", regex=True)
        out["decade_missing_explainable_n"] = int(explainable_decade_missing.sum())
        out["decade_missing_explainable_rate"] = float(explainable_decade_missing.mean()) if n else 0.0

    return out


# =========================
# Step 3.4 — Evidence log export & reproducible samples (with sample_purpose)
# =========================

def build_evidence_log(df: pd.DataFrame) -> pd.DataFrame:
    cols_wanted = [
        # Provenance
        "entry_id", "source_dataset", "source_record_id", "verification_url",
        # Identity/label
        "name_raw", "name_canonical",
        # Time
        "decades_norm", "time_basis", "time_precision_class", "start_year", "end_year",
        # Space
        "address_raw", "address_kind", "lat", "lon", "space_basis", "spatial_precision_class",
        # Geocode trace
        "geocode_attempted", "geocode_provider", "geocode_status", "geocode_skip_reason",
        "geocode_match_type", "geocode_matched_address", "geocode_query",
        # Corroboration
        "corroboration_status",
        # Manual audit control
        "audit_stage", "audit_decision", "audit_reason", "audit_flags",
        # Optional scope fields
        "scope_status", "scope_reason",
        # Notes
        "audit_note"
    ]
    keep = [c for c in cols_wanted if c in df.columns]
    ev = df[keep].copy()
    if "entry_id" in ev.columns:
        ev = ev.sort_values("entry_id", kind="stable")
    return ev


def _pick_rows(df: pd.DataFrame, mask: pd.Series, k: int, rng: np.random.Generator) -> List[int]:
    idx = df.index[mask].tolist()
    if not idx:
        return []
    take = min(k, len(idx))
    return rng.choice(idx, size=take, replace=False).tolist()


def sample_table4_rows_with_purpose(evidence_log: pd.DataFrame) -> pd.DataFrame:
    """
    Table 4 sample rows:
    - 1–2 rows per key branch
    - plus boundary cases
    - add sample_purpose column (one primary purpose per sampled row)
    """
    rng = np.random.default_rng(RANDOM_SEED)
    picks_with_purpose: List[Tuple[int, str]] = []

    # Branches to represent
    time_classes = sorted(set(evidence_log.get("time_precision_class", pd.Series([], dtype=str)).dropna().astype(str)))
    space_classes = sorted(set(evidence_log.get("spatial_precision_class", pd.Series([], dtype=str)).dropna().astype(str)))

    # 1) time_precision_class branches
    if "time_precision_class" in evidence_log.columns:
        for cls in time_classes:
            mask = evidence_log["time_precision_class"].astype(str) == cls
            for i in _pick_rows(evidence_log, mask, SAMPLES_PER_KEY_BRANCH, rng):
                picks_with_purpose.append((i, f"time_precision_class={cls}"))

    # 2) spatial_precision_class branches
    if "spatial_precision_class" in evidence_log.columns:
        for cls in space_classes:
            mask = evidence_log["spatial_precision_class"].astype(str) == cls
            for i in _pick_rows(evidence_log, mask, SAMPLES_PER_KEY_BRANCH, rng):
                picks_with_purpose.append((i, f"spatial_precision_class={cls}"))

    # 3) boundary: geocode_status=no_match (normalized value)
    if "geocode_status" in evidence_log.columns:
        mask = evidence_log["geocode_status"].fillna("").astype(str).str.lower() == "no_match"
        for i in _pick_rows(evidence_log, mask, SAMPLES_PER_KEY_BRANCH, rng):
            picks_with_purpose.append((i, "boundary_geocode_no_match"))

    # 4) boundary: address_kind=intersection
    if "address_kind" in evidence_log.columns:
        mask = evidence_log["address_kind"].fillna("").astype(str).str.lower() == "intersection"
        for i in _pick_rows(evidence_log, mask, SAMPLES_PER_KEY_BRANCH, rng):
            picks_with_purpose.append((i, "boundary_address_kind_intersection"))

    # 5) boundary: coords_missing
    if "lat" in evidence_log.columns and "lon" in evidence_log.columns:
        lat = pd.to_numeric(evidence_log["lat"], errors="coerce")
        lon = pd.to_numeric(evidence_log["lon"], errors="coerce")
        mask = pd.isna(lat) | pd.isna(lon)
        for i in _pick_rows(evidence_log, mask, SAMPLES_PER_KEY_BRANCH, rng):
            picks_with_purpose.append((i, "boundary_coords_missing"))

    # 6) boundary: decade_missing
    if "decades_norm" in evidence_log.columns:
        decades_list = evidence_log["decades_norm"].apply(safe_json_loads_list)
        has_decade = decades_list.apply(lambda v: isinstance(v, list) and len(v) > 0)
        mask = ~has_decade
        for i in _pick_rows(evidence_log, mask, SAMPLES_PER_KEY_BRANCH, rng):
            picks_with_purpose.append((i, "boundary_decade_missing"))

    # Deduplicate while keeping first purpose
    seen: set[int] = set()
    final_rows: List[Tuple[int, str]] = []
    for i, purpose in picks_with_purpose:
        if i not in seen:
            seen.add(i)
            final_rows.append((i, purpose))

    sample = evidence_log.loc[[i for i, _ in final_rows]].copy()
    sample.insert(0, "sample_purpose", [p for _, p in final_rows])
    return sample


# =========================
# Main
# =========================

def main() -> None:
    if not IN_PATH.exists():
        raise FileNotFoundError(f"Input not found: {IN_PATH}")

    df = pd.read_csv(IN_PATH)

    # Minimal required columns for auditability
    required = ["entry_id", "source_dataset", "source_record_id"]
    missing = require_cols(df, required)
    if missing:
        raise ValueError(f"Missing required columns: {missing}")

    # -------------------------
    # Step 3.1 — Derivation
    # -------------------------
    if "time_precision_class" not in df.columns:
        df["time_precision_class"] = derive_time_precision_class(df)

    if "spatial_precision_class" not in df.columns:
        df["spatial_precision_class"] = derive_spatial_precision_class(df)

    if "corroboration_status" not in df.columns:
        df["corroboration_status"] = derive_corroboration_status(df)

    # -------------------------
    # Representation normalization (non-interpretive)
    # -------------------------
    # Normalize enum casing for auditability; do not change geocoding outcomes.
    if "geocode_status" in df.columns:
        df["geocode_status"] = df["geocode_status"].apply(normalize_geocode_status)

    # Add manual audit control fields (new)
    df = add_manual_audit_control_fields(df)

    # -------------------------
    # Step 3.2 — Consistency checks
    # -------------------------
    issues: List[Dict[str, Any]] = []
    issues.extend(validate_enums(df))
    issues.extend(validate_logic(df))

    if issues:
        pd.DataFrame(issues).to_csv(OUT_VALIDATION_ISSUES, index=False)
    else:
        if OUT_VALIDATION_ISSUES.exists():
            OUT_VALIDATION_ISSUES.unlink()

    # -------------------------
    # Step 3.3 — Missingness audit
    # -------------------------
    miss_summary = missingness_audit(df)

    # Distributions (for reporting) — avoid turning NaN into 'nan'
    dist: Dict[str, Any] = {}
    for col in [
        "time_precision_class", "spatial_precision_class",
        "geocode_status", "address_kind", "time_basis", "space_basis",
        "audit_stage", "audit_decision"
    ]:
        if col in df.columns:
            dist[f"{col}_counts"] = value_counts_for_json(df[col])

    # -------------------------
    # Step 3.4 — Evidence log export & samples
    # -------------------------
    evidence_log = build_evidence_log(df)
    evidence_log.to_csv(OUT_EVIDENCE_LOG, index=False)

    table4 = sample_table4_rows_with_purpose(evidence_log)
    table4.to_csv(OUT_TABLE4_SAMPLES, index=False)

    summary = {
        "generated_at_utc": now_iso(),
        "input_path": str(IN_PATH),
        "outputs": {
            "evidence_log": str(OUT_EVIDENCE_LOG),
            "table4_sample_rows": str(OUT_TABLE4_SAMPLES),
            "validation_issues": str(OUT_VALIDATION_ISSUES) if issues else None,
        },
        "missingness_summary": miss_summary,
        "distributions": dist,
        "issues_summary": {
            "n_issues": int(len(issues)),
            "n_errors": int(sum(1 for x in issues if x.get("severity") == "error")),
            "n_warnings": int(sum(1 for x in issues if x.get("severity") == "warning")),
        },
        "issue_examples": issues[:10],
    }

    with open(OUT_VALIDATION_SUMMARY, "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    # Fail-fast on errors
    n_errors = summary["issues_summary"]["n_errors"]
    if n_errors > 0:
        raise RuntimeError(
            f"Validation failed with {n_errors} error(s). "
            f"See {OUT_VALIDATION_ISSUES} and {OUT_VALIDATION_SUMMARY}."
        )


if __name__ == "__main__":
    main()